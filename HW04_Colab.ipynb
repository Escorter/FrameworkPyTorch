{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "icxKAKdfdj-m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "GbNFGHGmdv4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20f440ae-f048-4883-f5a3-5d4a32b4af81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.CIFAR100(root='data/', train=True, download=True)\n",
        "\n",
        "def train_valid_split(Xt):\n",
        "    X_train, X_test = train_test_split(Xt, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test\n",
        "\n",
        "class MyOwnCifar(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, init_dataset, transform=None):\n",
        "        self._base_dataset = init_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self._base_dataset[idx][0]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, self._base_dataset[idx][1]"
      ],
      "metadata": {
        "id": "L12mUD-fdv2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7ab40f-60dc-4071-ee94-8bdb925786cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 40804703.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_actions = transforms.Compose([\n",
        "                                    transforms.RandomGrayscale(0.2),\n",
        "                                    transforms.ColorJitter(brightness=.3, hue=.4),\n",
        "                                    transforms.RandomRotation(degrees=(0,5)),\n",
        "                                    transforms.ToTensor()])\n",
        "\n",
        "train_dataset, valid_dataset = train_valid_split(dataset)\n",
        "\n",
        "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
        "valid_dataset = MyOwnCifar(valid_dataset, transforms.ToTensor())"
      ],
      "metadata": {
        "id": "s5dhanlZdvzS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2)"
      ],
      "metadata": {
        "id": "c21Po_1Xdvwa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.dp_one = nn.Dropout(0.2)\n",
        "        self.dp_two = nn.Dropout(0.25)\n",
        "\n",
        "        self.bn_one = torch.nn.BatchNorm2d(3)\n",
        "        self.conv_one = torch.nn.Conv2d(3, 60, 3, padding=1)\n",
        "        self.bn_two = torch.nn.BatchNorm2d(60)\n",
        "        self.conv_two = torch.nn.Conv2d(60, 120, 3, padding=1)\n",
        "        self.bn_three = torch.nn.BatchNorm2d(120)\n",
        "        self.conv_three = torch.nn.Conv2d(120, 240, 3, padding=1)\n",
        "        self.bn_four = torch.nn.BatchNorm2d(240)\n",
        "        self.fc1 = torch.nn.Linear(3840, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 256)\n",
        "        self.out = torch.nn.Linear(256, 100)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn_one(x)\n",
        "        x = self.conv_one(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.bn_two(x)\n",
        "        x = self.conv_two(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.bn_three(x)\n",
        "        x = self.conv_three(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.bn_four(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dp_one(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dp_two(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return self.out(x)\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "-bKFcYHodvti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53b82b4-cdf0-4cb7-b9f8-3847f7bdc638"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (dp_one): Dropout(p=0.2, inplace=False)\n",
            "  (dp_two): Dropout(p=0.25, inplace=False)\n",
            "  (bn_one): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_one): Conv2d(3, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_two): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_two): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_three): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_three): Conv2d(120, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn_four): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=3840, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (out): Linear(in_features=256, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "6TcorJPkdvqj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net.to(device), input_size=(3, 32, 32))"
      ],
      "metadata": {
        "id": "onzsDoYJdvnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97895592-3692-4daf-b797-83c6aaf25d90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 32, 32]               6\n",
            "            Conv2d-2           [-1, 60, 32, 32]           1,680\n",
            "       BatchNorm2d-3           [-1, 60, 16, 16]             120\n",
            "            Conv2d-4          [-1, 120, 16, 16]          64,920\n",
            "       BatchNorm2d-5            [-1, 120, 8, 8]             240\n",
            "            Conv2d-6            [-1, 240, 8, 8]         259,440\n",
            "       BatchNorm2d-7            [-1, 240, 4, 4]             480\n",
            "           Dropout-8                 [-1, 3840]               0\n",
            "            Linear-9                 [-1, 1024]       3,933,184\n",
            "          Dropout-10                 [-1, 1024]               0\n",
            "           Linear-11                  [-1, 256]         262,400\n",
            "           Linear-12                  [-1, 100]          25,700\n",
            "================================================================\n",
            "Total params: 4,548,170\n",
            "Trainable params: 4,548,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.10\n",
            "Params size (MB): 17.35\n",
            "Estimated Total Size (MB): 18.46\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 7\n",
        "net.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        running_loss += loss.item()\n",
        "        running_items += len(labels)\n",
        "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        if (i != 0) & (i % 100 == 0):    # печатаем каждые 100 mini-batches\n",
        "            net.eval()\n",
        "\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
        "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "                  f'Loss: {running_loss / running_items:.3f}. ' \\\n",
        "                  f'Train acc: {running_right / running_items:.3f}', end='. ')\n",
        "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "\n",
        "            test_running_right, test_running_total = 0.0, 0.0\n",
        "            for i, data in enumerate(valid_loader):\n",
        "\n",
        "                test_outputs = net(data[0].to(device))\n",
        "                test_running_total += len(data[1])\n",
        "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum()\n",
        "\n",
        "            print(f'Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "        net.train()\n",
        "\n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "id": "eDchejBOdvkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fa860d-dda3-4061-f283-cf92f3375268"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/7]. Step [101/313]. Loss: 0.033. Train acc: 0.067. Test acc: 0.113\n",
            "Epoch [1/7]. Step [201/313]. Loss: 0.030. Train acc: 0.130. Test acc: 0.158\n",
            "Epoch [1/7]. Step [301/313]. Loss: 0.028. Train acc: 0.166. Test acc: 0.217\n",
            "Epoch [2/7]. Step [101/313]. Loss: 0.025. Train acc: 0.221. Test acc: 0.249\n",
            "Epoch [2/7]. Step [201/313]. Loss: 0.024. Train acc: 0.252. Test acc: 0.280\n",
            "Epoch [2/7]. Step [301/313]. Loss: 0.023. Train acc: 0.271. Test acc: 0.306\n",
            "Epoch [3/7]. Step [101/313]. Loss: 0.021. Train acc: 0.316. Test acc: 0.334\n",
            "Epoch [3/7]. Step [201/313]. Loss: 0.021. Train acc: 0.336. Test acc: 0.341\n",
            "Epoch [3/7]. Step [301/313]. Loss: 0.021. Train acc: 0.335. Test acc: 0.364\n",
            "Epoch [4/7]. Step [101/313]. Loss: 0.019. Train acc: 0.383. Test acc: 0.375\n",
            "Epoch [4/7]. Step [201/313]. Loss: 0.018. Train acc: 0.394. Test acc: 0.386\n",
            "Epoch [4/7]. Step [301/313]. Loss: 0.018. Train acc: 0.390. Test acc: 0.400\n",
            "Epoch [5/7]. Step [101/313]. Loss: 0.016. Train acc: 0.456. Test acc: 0.413\n",
            "Epoch [5/7]. Step [201/313]. Loss: 0.016. Train acc: 0.449. Test acc: 0.411\n",
            "Epoch [5/7]. Step [301/313]. Loss: 0.016. Train acc: 0.440. Test acc: 0.418\n",
            "Epoch [6/7]. Step [101/313]. Loss: 0.014. Train acc: 0.516. Test acc: 0.439\n",
            "Epoch [6/7]. Step [201/313]. Loss: 0.014. Train acc: 0.499. Test acc: 0.433\n",
            "Epoch [6/7]. Step [301/313]. Loss: 0.014. Train acc: 0.506. Test acc: 0.442\n",
            "Epoch [7/7]. Step [101/313]. Loss: 0.012. Train acc: 0.567. Test acc: 0.442\n",
            "Epoch [7/7]. Step [201/313]. Loss: 0.013. Train acc: 0.559. Test acc: 0.445\n",
            "Epoch [7/7]. Step [301/313]. Loss: 0.013. Train acc: 0.547. Test acc: 0.451\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resnet50**"
      ],
      "metadata": {
        "id": "6PWd6auyfGEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "print(resnet50)"
      ],
      "metadata": {
        "id": "1uIfLAszdvh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "set_parameter_requires_grad(resnet50, True)\n",
        "resnet50.fc = nn.Linear(2048, 100)\n",
        "summary(resnet50.to(device), input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "NSbphXd4dvfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = resnet50.to(device)"
      ],
      "metadata": {
        "id": "WcFy0xy-dvb6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_actions = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.RandomCrop(224, padding=4),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                         std=[0.229, 0.224, 0.225])])\n",
        "valid_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                            std=[0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "wip0aMOTdvZC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, valid_dataset = train_valid_split(dataset)\n",
        "\n",
        "train_dataset = MyOwnCifar(train_dataset, train_actions)\n",
        "valid_dataset = MyOwnCifar(valid_dataset, valid_transforms)"
      ],
      "metadata": {
        "id": "csx_ZaN9fDFb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2)"
      ],
      "metadata": {
        "id": "89r5yWFMfDAj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name, param in resnet50.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "mVWRehmjfC9T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 7\n",
        "resnet50.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        running_loss += loss.item()\n",
        "        running_items += len(labels)\n",
        "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
        "\n",
        "        # выводим статистику о процессе обучения\n",
        "        if (i != 0) & (i % 100 == 0):    # печатаем каждые 100 mini-batches\n",
        "            resnet50.eval()\n",
        "\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
        "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
        "                  f'Loss: {running_loss / running_items:.3f}. ' \\\n",
        "                  f'Train acc: {running_right / running_items:.3f}', end='. ')\n",
        "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
        "\n",
        "            test_running_right, test_running_total = 0.0, 0.0\n",
        "            for i, data in enumerate(valid_loader):\n",
        "\n",
        "                test_outputs = resnet50(data[0].to(device))\n",
        "                test_running_total += len(data[1])\n",
        "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum()\n",
        "\n",
        "            print(f'Test acc: {test_running_right / test_running_total:.3f}')\n",
        "\n",
        "        resnet50.train()\n",
        "\n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaddK7olfm9q",
        "outputId": "1202816f-2683-4f9a-b8a9-029478380c8b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/7]. Step [101/313]. Loss: 0.026. Train acc: 0.259. Test acc: 0.425\n",
            "Epoch [1/7]. Step [201/313]. Loss: 0.018. Train acc: 0.445. Test acc: 0.504\n",
            "Epoch [1/7]. Step [301/313]. Loss: 0.016. Train acc: 0.489. Test acc: 0.520\n",
            "Epoch [2/7]. Step [101/313]. Loss: 0.014. Train acc: 0.523. Test acc: 0.543\n",
            "Epoch [2/7]. Step [201/313]. Loss: 0.014. Train acc: 0.528. Test acc: 0.543\n",
            "Epoch [2/7]. Step [301/313]. Loss: 0.013. Train acc: 0.533. Test acc: 0.558\n",
            "Epoch [3/7]. Step [101/313]. Loss: 0.013. Train acc: 0.553. Test acc: 0.553\n",
            "Epoch [3/7]. Step [201/313]. Loss: 0.013. Train acc: 0.557. Test acc: 0.565\n",
            "Epoch [3/7]. Step [301/313]. Loss: 0.013. Train acc: 0.558. Test acc: 0.569\n",
            "Epoch [4/7]. Step [101/313]. Loss: 0.012. Train acc: 0.575. Test acc: 0.573\n",
            "Epoch [4/7]. Step [201/313]. Loss: 0.012. Train acc: 0.573. Test acc: 0.580\n",
            "Epoch [4/7]. Step [301/313]. Loss: 0.012. Train acc: 0.577. Test acc: 0.584\n",
            "Epoch [5/7]. Step [101/313]. Loss: 0.012. Train acc: 0.593. Test acc: 0.582\n",
            "Epoch [5/7]. Step [201/313]. Loss: 0.012. Train acc: 0.582. Test acc: 0.579\n",
            "Epoch [5/7]. Step [301/313]. Loss: 0.012. Train acc: 0.583. Test acc: 0.589\n",
            "Epoch [6/7]. Step [101/313]. Loss: 0.011. Train acc: 0.603. Test acc: 0.585\n",
            "Epoch [6/7]. Step [201/313]. Loss: 0.011. Train acc: 0.595. Test acc: 0.583\n",
            "Epoch [6/7]. Step [301/313]. Loss: 0.012. Train acc: 0.592. Test acc: 0.591\n",
            "Epoch [7/7]. Step [101/313]. Loss: 0.011. Train acc: 0.604. Test acc: 0.584\n",
            "Epoch [7/7]. Step [201/313]. Loss: 0.011. Train acc: 0.593. Test acc: 0.586\n",
            "Epoch [7/7]. Step [301/313]. Loss: 0.011. Train acc: 0.604. Test acc: 0.592\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kuC2Ijygfm7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEPp5I5pfm4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZ_jPf3Zfm1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDQgwGbOfmy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOnON19BfmwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNCla-6ofC6r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}